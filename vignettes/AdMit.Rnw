\documentclass[nojss,shortnames]{jss}
\usepackage{amsmath,amssymb}
\usepackage{xspace}
\usepackage{Sweave}

%% general
\providecommand{\AdMit}{\pkg{AdMit}\xspace}
\providecommand{\ARCH}{\ensuremath{\text{ARCH}}\xspace}
\providecommand{\CI}{\ensuremath{\text{CI}}\xspace}
\providecommand{\DEoptim}{\pkg{DEoptim}\xspace}
\providecommand{\GARCH}{\ensuremath{\text{GARCH}}\xspace}
\providecommand{\GG}{Griddy-Gibbs\xspace}
\providecommand\MCMC{MCMC\xspace}
\providecommand\MH{M-H\xspace}
\providecommand\NSE{\ensuremath{\text{NSE}}\xspace}
\providecommand\RNE{\ensuremath{\text{RNE}}\xspace}
\providecommand\Student{\ensuremath{\text{Student-}t}\xspace}

%% sectionning etc.
\providecommand{\myFigure}[1]{Figure~\ref{#1}\xspace}
\providecommand{\myTable}[1]{Table~\ref{#1}\xspace}
\providecommand{\mySection}[1]{Section~\ref{#1}\xspace}

%% distributions
\providecommand{\No}{\ensuremath{\mathcal{N}}\xspace}
\providecommand{\dNo}{\ensuremath{\phi}\xspace}
\providecommand{\St}{\ensuremath{\mathcal{S}}\xspace}
\providecommand{\dSt}{\ensuremath{t}\xspace}
\providecommand{\Un}{\ensuremath{\mathcal{U}}\xspace}

%% maths
\renewcommand\geq{\ensuremath{\geqslant}}
\renewcommand\leq{\ensuremath{\leqslant}}
\providecommand\myBigDot{\ensuremath{\bullet}}
\providecommand\half{\ensuremath{\tfrac 1 2}}
\providecommand\Half{\ensuremath{\frac 1 2}}
\providecommand\e{\ensuremath{\varepsilon}}
\providecommand\point{\ensuremath{\hspace{.1cm}.}}
\providecommand\comma{\ensuremath{\hspace{.1cm},}}
\providecommand\iid{\overset{\textit{i.i.d.}}{\sim}}
\providecommand\alphab{\ensuremath{\boldsymbol{\mathbf \alpha}}\xspace}
\providecommand\yb{\ensuremath{\boldsymbol{\mathbf y}}\xspace}
\providecommand\thetab{\ensuremath{\boldsymbol{\theta}}\xspace}
\providecommand\mub{\ensuremath{\boldsymbol{\mu}}\xspace}
\providecommand\Sigmab{\ensuremath{\boldsymbol{\Sigma}}\xspace}
\providecommand{\Ind}[1]{\ensuremath{\mathbb{I}_{\{#1\}}}\xspace}
\providecommand{\vecr}[4]{\ensuremath{(#1_{#2#31} \cdots #1_{#2#3#4})}\xspace}
\providecommand{\ve}[1]{\ensuremath{\vecr{#1}{\empty}{\empty}{T}}\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TITLE PAGE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{David Ardia\\HEC Montreal\And
        Lennart F. Hoogerheide\\Vrije Universit\"at Amsterdam\And
        Herman K. van Dijk\\Erasmus University}
\Plainauthor{David Ardia, Lennart F. Hoogerheide, Herman K. van Dijk}

\title{Adaptive Mixture of \Student\\ Distributions as a Flexible Candidate Distribution for
Efficient Simulation: The \proglang{R} Package \pkg{AdMit}}
\Plaintitle{Adaptive Mixture of Student-t Distributions: The R Package AdMit} 
\Shorttitle{Adaptive Mixture of Student-t Distributions: The R Package AdMit}

\Abstract{%
This introduction to the \proglang{R} package \AdMit is a shorter version
of \citet{Ardia:Hoogerheide:VanDijk:09}, published in the \emph{Journal of Statistical Software}.
The package provides flexible functions to approximate 
a certain target distribution and to efficiently generate a sample of random draws from it, 
given only a kernel of the target density function. 
The core algorithm consists of the function \code{AdMit} which fits an adaptive mixture
of \Student distributions to the density of interest. Then, importance sampling
or the independence chain Metropolis-Hastings algorithm is used to obtain quantities of interest for
the target density, using the fitted mixture as the importance or candidate density. 
The estimation procedure is fully automatic and thus avoids the time-consuming and
difficult task of tuning a sampling algorithm. The relevance of the package is shown
in an example of a bivariate bimodal distribution.
}

\Keywords{adaptive mixture, \Student distributions, importance sampling, independence
chain Metropolis-Hastings algorithm, Bayesian inference, \proglang{R} software}

\Plainkeywords{adaptive mixture, Student's t distributions, importance sampling, independence 
chain Metropolis-Hastings algorithm, Bayesian inference, R software} 

\Address{
  David Ardia\\
  Departement of Decision Sciences\\
  HEC Montreal, Canada\\
  E-mail: \email{david.ardia.ch@gmail.com}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MAIN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\SweaveOpts{concordance=TRUE}

\SweaveOpts{engine=R,eps=FALSE}
%\VignetteIndexEntry{Adaptive Mixture of Student-t Distributions for Efficient Simulation}
%\VignetteDepends{AdMit,mvtnorm,coda}
%\VignetteKeywords{adaptive, mixture, Student, importance sampling, Metropolis-Hastings, Bayesian, R}
%\VignettePackage{AdMit}

<<echo=FALSE,results=hide>>=
rm(list = ls())
library("AdMit")
options(digits = 4, max.print = 40, prompt = "R> ", continue = "+   ")
@

\section{Introduction}
\label{s:INTRO}

In scientific analysis one is usually interested in the effect of one variable, say, education $(=x)$, on an
other variable, say, earned income $(=y)$. In the standard linear regression model this effect of $x$ on $y$ is
assumed constant, i.e., $\E(y)=\beta x$, with $\beta$ a constant. The uncertainty of many estimators of $\beta$ is usually
represented by a symmetric \Student density \citep[see, e.g.,][Chapter~3]{Heij:DeBoer:Franses:Kloek:VanDijk:04}. 
However, in many realistic models the effect of $x$ on $y$ is a function of several deeper structural parameters.
In such cases, the uncertainty of the estimates of $\beta$ may be rather non-symmetric. More formally, in a Bayesian procedure, the 
target or posterior density may exhibit rather non-elliptical 
shapes \citep[see, e.g.,][]{Hoogerheide:Kaashoek:VanDijk:07,Hoogerheide:VanDijk:08}. Hence, in several cases of 
scientific analysis, one deals with a target distribution that has 
very non-elliptical contours and that it is not a member of a known 
class of distributions. Therefore, there exists a need for flexible and efficient 
simulation methods to approximate such target distributions.

This article illustrates the adaptive mixture of \Student distributions (AdMit)
procedure \citep[see][for details]{Hoogerheide:06,Hoogerheide:Kaashoek:VanDijk:07,Hoogerheide:VanDijk:08} 
and presents its \proglang{R} implementation \citep{R} with the 
package \AdMit \citep{AdMit}. The AdMit procedure 
consists of the construction of a mixture of \Student distributions which
approximates a target distribution of interest. The fitting procedure relies only on a kernel of 
the target density, so that the normalizing constant is not required. 
In a second step this approximation is used as an importance function in importance sampling 
or as a candidate density in the independence chain Metropolis-Hastings (\MH) algorithm 
to estimate characteristics of the target density. The estimation procedure is fully automatic 
and thus avoids the difficult task, especially for non-experts, of tuning a sampling algorithm. 
The \proglang{R} package \AdMit is available from the Comprehensive \proglang{R} Archive Network 
at \url{http://CRAN.R-project.org/package=AdMit}.

In a \emph{standard} case of importance sampling or the independence chain \MH algorithm, the candidate density
is unimodal. If the target distribution is multimodal then some draws may have 
huge weights in the importance sampling approach and a second mode may be
completely missed in the \MH strategy. As a consequence, the
convergence behavior of these Monte Carlo integration methods is rather uncertain. Thus, an important 
problem is the choice
of the importance or candidate density, especially when little is known a priori about the shape of 
the target density. For both importance sampling and the independence chain \MH, it holds that 
the candidate density should
be \emph{close} to the target density, and it is especially important that the tails of the candidate
should not be thinner than those of the target.

\citet{Hoogerheide:06} and \citet{Hoogerheide:Kaashoek:VanDijk:07} 
mention several reasons why mixtures 
of \Student distributions are natural candidate densities. First, they
can provide an accurate approximation to a wide variety of target densities, with substantial 
skewness and high kurtosis. Furthermore, they can
deal with multi-modality and with non-elliptical shapes due to asymptotes. Second, this 
approximation can be constructed in a quick, iterative procedure and 
a mixture of \Student distributions is easy to sample from. Third, the \Student distribution has fatter
tails than the Normal distribution; especially if one specifies \Student distributions with few degrees
of freedom, the risk is small that the tails of the candidate are thinner than those of the target
distribution. Finally, \citet{Zeevi:Meir:97} showed that under certain conditions any 
density function may be approximated to arbitrary accuracy by
a convex combination of \emph{basis} densities; the mixture of \Student distributions 
falls within their framework. One sufficient condition ensuring the feasibility of the approach 
is that the target density function is continuous on a compact domain. It is further allowed
that the target density is not defined on a compact set, but with tails behaving like a \Student
distribution. Furthermore, it is even allowed that the target tends to infinity at a certain value
as long as the function is square integrable. In practice, a non-expert user sometimes does not know
whether the necessary conditions are satisfied. However, one can check the behaviour of the relative
numerical efficiency as robustness check; if the necessary conditions are not satisfied, this will tend
to zero as the number of draws increases (even if the number of components in the approximation becomes
larger). Obviously, if the provided target density kernel does not correspond to a proper distribution,
the approximation will not converge to a sensible result. These cases of improper distributions should be 
discovered before starting a Monte Carlo simulation.  

The \proglang{R} package \AdMit consists of three main 
functions: \code{AdMit}, \code{AdMitIS} and \code{AdMitMH}. The
first one allows the user to fit a mixture of \Student distributions to a 
given density through its kernel function. The next two functions perform importance 
sampling and independence chain \MH sampling using the fitted mixture 
estimated by \code{AdMit} as the importance or candidate density, respectively. 
To illustrate the use of the package, we first apply the AdMit 
methodology to a bivariate bimodal distribution. We describe in 
detail the use of the functions provided by the package and document the relevance 
of the methodology to reproduce the shape of non-elliptical
distributions.

The outline of the paper is as follows: \mySection{s:ADMIT} presents the principles of the AdMit 
algorithm. \mySection{s:GM} presents the functions provided by the package with an 
illustration of a bivariate non-elliptical distribution. \mySection{s:CONCLUSION} concludes.

\section[Adaptive mixture of Student's t distributions]{Adaptive mixture of \Student distributions}
\label{s:ADMIT}

The adaptive mixture of \Student distributions method 
developed in \citet{Hoogerheide:06} and \citet{Hoogerheide:Kaashoek:VanDijk:07} constructs
a mixture of \Student distributions in order to
approximate a given target density $p(\thetab)$ 
where $\thetab \in \Theta \subseteq \mathbb{R}^d$. The density of a mixture 
of \Student distributions can be written as:
\[
q(\thetab) = \sum_{h=1}^H \eta_h \, \dSt_d(\thetab \,|\, \mub_h, \Sigmab_h, \nu)
\comma
\]
where $\eta_h$ $(h=1,\ldots,H)$ are the mixing probabilities of the
\Student components, $0 \leq \eta_h \leq 1$ $(h=1,\ldots,H)$, $\sum_{h=1}^H \eta_h = 1$, and
$\dSt_d(\thetab \,|\, \mub_h, \Sigmab_h, \nu)$ is a $d$-dimensional \Student density with mode vector
$\mub_h$, scale matrix $\Sigmab_h$, and $\nu$ degrees of freedom:
\begin{align*}
\dSt_d(\thetab \,|\, \mub_h, \Sigmab_h, \nu)
& = 
\frac{\Gamma\left(\frac{\nu+d}{2}\right)}{\Gamma\left( \frac{\nu}{2} \right)(\pi \nu)^{d/2}}\\
& \times (\det \Sigmab_h)^{-1/2}
\left(
1 + \frac{(\thetab-\mub_h)' \Sigmab_h^{-1} (\thetab-\mub_h)}{\nu}
\right)^{-(\nu+d)/2}
\point
\end{align*}

The adaptive mixture approach determines $H$, $\eta_h$, $\mub_h$ and $\Sigmab_h$ $(h=1,\ldots,H)$ based
on a kernel function $k(\thetab)$ of the target density $p(\thetab)$. It consists of
the following steps:
\begin{description}
\item[Step 0 -- Initial step] Compute the mode $\mub_1$ and
scale $\Sigmab_1$ of the first \Student distribution in the mixture
as $\mub_1 = \arg \max_{\thetab \in \Theta} \log k(\thetab)$, the mode of the log kernel function, and $\Sigmab_1$
as minus the Hessian of $\log k(\thetab)$ evaluated at its mode $\mub_1$. Then draw a set 
of $N_s$ points $\thetab^{[i]}$ $(i=1,\ldots,N_s)$ from this first stage candidate density 
$q(\thetab) = \dSt_d(\thetab \,|\, \mub_1, \Sigmab_1, \nu)$, with small $\nu$ to allow for fat tails.

\emph{Comment:} In the rest of this paper, we use \Student distributions 
with one degrees of freedom (i.e., $\nu=1$) since:
\begin{enumerate}
\item it enables the method to deal with fat-tailed target distributions;
\item it makes it easier for the iterative procedure to detect modes that are far apart.
\end{enumerate}

After that add components to the mixture, iteratively, by performing the following steps:
\item[Step 1 -- Evaluate the distribution of weights] Compute the importance sampling
weights $w(\thetab^{[i]}) = k(\thetab^{[i]})/q(\thetab^{[i]})$ for $i=1,\ldots,N_s$. In order to 
determine the number of components $H$ of the mixture we make use of a simple
diagnostic criterion: \emph{the coefficient of variation}, i.e., the standard deviation divided by the mean,
of the importance sampling weights $\{w(\thetab^{[i]}) \,|\, i=1,\ldots,N_s\}$. If the relative change in the coefficient of
variation of the importance sampling weights caused by adding one new \Student component to the candidate mixture is small, e.g.,
less than 10\%, then the algorithm stops and the current mixture $q(\thetab)$ is the approximation. Otherwise, the algorithm
goes to step 2.

\emph{Comment:} Notice that $q(\thetab)$ is a proper density, whereas $k(\thetab)$ is a density kernel. So, the
procedure does not provide an approximation to the kernel $k(\thetab)$ but provides an 
approximation to the density of which $k(\thetab)$ is a kernel. 

\emph{Comment:} There are several reasons for using the coefficient of variation of the importance sampling weights.
First, it is a natural, intuitive measure of quality of the candidate as an approximation to 
the target. If the candidate and the target distributions coincide, all importance 
sampling weights are equal, so that the coefficient of 
variation is zero. For a poor candidate that not even roughly approximates the target, some importance 
sampling weights are huge while most are (almost) 
zero, so that the coefficient
of variation is high. The better the candidate approximates the target, the more evenly 
the weight is divided among the
candidate draws, and the smaller the coefficient of variation of the 
importance sampling weights. Second, \citet{Geweke:89} argues that a 
reasonable objective in the choice of an importance 
density is the minimization of:
\[
\E_p[w(\thetab)]
= \int \frac{k(\thetab)^2}{q(\thetab)} \text{d} \thetab
= \int \left[ \frac{k(\thetab)}{q(\thetab)} \right]^2 q(\thetab) \text{d} \thetab
= \E_q [ w(\thetab)^2 ] \comma
\]
or equivalently, the minimization of the coefficient of variation:
\[
\frac{ \big( \E_q[w(\thetab)^2] - \E_q[w(\thetab)]^2 \big)^{1/2} }
{\E_q[w(\thetab)]} \comma
\]
since:
\[
\E_q[w(\thetab)]
= \int \frac{k(\thetab)}{q(\thetab)} q(\thetab) \text{d} \thetab
= \int k(\thetab) \text{d} \thetab
\]
does not depend on $q(\thetab)$.

The reason for quoting the coefficient of variation rather than the standard deviation
is that the standard deviation of the \emph{scaled} weights (i.e., adding up to one) depends
on the number of draws, whereas the standard deviation of the \emph{unscaled} weights depends on the
scaling constant $\int k(\thetab) \text{d}\thetab$ (i.e., typically the marginal likelihood). The coefficient
of variation of the importance sampling weights, which is equal for scaled and unscaled weights,
reflects the quality of the candidate as an approximation to the target (not depending on number of draws
or $\int k(\thetab) \text{d}\thetab$). The coefficient of variation is the function one would minimize if
one desires to estimate $\Prob(\thetab \in D)$, where $D \subset \Theta$, if the
true value is $\Prob(\thetab \in D) = 0.5$. Different functions should be 
minimized for different quantities of interest. 
However, it is usually impractical to perform a separate tuning algorithm for the importance density
for each quantity of interest. Fortunately, in practice the candidate resulting from the 
minimization of the coefficient of variation performs well for estimating common quantities of interest
such as posterior moments. \citet{Hoogerheide:VanDijk:08:2} propose a different approach for
forecasting extreme quantiles where one substantially improves on the usual strategy by generating 
relatively far too many extreme candidate draws. 

\item[Step 2a -- Iterate on the number of components] Add another \Student distribution with 
density $\dSt_d(\thetab \,|\, \mub_h, \Sigmab_h, \nu)$ to the 
mixture with $\mub_h = \arg \max_{\thetab \in \Theta} \log w(\thetab)$ and
$\Sigmab_h$ equal to minus the inverse Hessian of $\log w(\thetab)$. Here, $q(\thetab)$
denotes the density of the mixture of $(h-1)$ \Student distributions obtained 
in the previous iteration of the procedure. An obvious initial value for the maximization procedure for 
computing $\mub_h$ is the point $\thetab^{[i]}$ with the highest 
weight in the sample $\{w(\thetab^{[i]}) \,|\, i=1,\ldots,N_s\}$.
The idea behind this choice is that the new \Student component should \emph{cover} a
region where the weights $w(\thetab)$ are relatively large. The point where the weight function $w(\thetab)$ attains
its maximum is an obvious choice for $\mub_h$, while the scale matrix $\Sigmab_h$ is the covariance matrix of the local
Normal approximation to the distribution with density kernel $w(\thetab)$ around the point $\mub_h$.

\emph{Comment:} There are several reasons for the use of minus the inverse Hessian of $\log w(\thetab)$ as the scale
matrix for the new component. First, suppose that $k(\thetab)$ is a posterior kernel under a flat prior, and that
the first candidate distribution would be a uniform distribution (or a \Student with a huge scale matrix). Then 
$\log w(\thetab)$ takes its maximum likelihood estimator and minus its inverse Hessian is
an asymptotically valid estimate for the maximum likelihood estimator's covariance matrix. 
Second, since $\log w(\thetab)$ takes its maximum at $\mub_h$, its Hessian is negative definite 
(unless it is located at a boundary, in which case we do not use this scale matrix). Therefore, minus the 
inverse Hessian is a positive definite matrix that can be used as a covariance or scale matrix. Moreover, we 
want to add candidate probability mass to those areas of the parameter space where $w(\thetab)$ is 
relatively high, i.e., where there is relatively little candidate probability mass. This is the reason for 
choosing the mode $\mub_h$ of the new candidate component at the maximum of $w(\thetab)$. Especially
in those directions where $w(\thetab)$ decreases slowy (i.e., moving away from $\mub_h$) we want to
add candidate probability mass also further away from $\mub_h$. This is reflected by larger elements of minus
the inverse Hessian of $\log w(\thetab)$ at $\mub_h$. Note that $w(\thetab)$ is
generally not a kernel of a proper density on $\Theta$. However, we also do not require this. We only make use
of its local behaviour around its maximum at $\mub_h$, reflected by minus the inverse Hessian 
of $\log w(\thetab)$. That is, we specify a \Student distribution that locally behaves the same
as the ratio $w(\thetab)$. 

\emph{Comment:} To improve the algorithm's ability to detect distant modes of a multimodal 
target density we consider one additional initial value for
the optimization and we use the point corresponding to the highest value of the 
weight function among the two optima as the mode $\mub_h$ of the new component in the candidate mixture. 

\item[Step 2b -- Optimize the mixing probabilities] Choose the probabilities $\eta_h$ $(h=1,\ldots,H)$ in the 
mixture $q(\thetab) = \sum_{h=1}^H \eta_h \, \dSt_d(\thetab \,|\, \mub_h, \Sigmab_h, \nu)$ by minimizing the (squared) coefficient
of variation of the importance sampling weights. First, draw $N_p$ points 
$\thetab_h^{[i]}$ $(i=1,\dots,N_p)$ from each component
$\dSt_d(\thetab \,|\, \mub_h, \Sigmab_h, \nu)$ $(h=1\ldots,H)$. Then, minimize:
\begin{align}~\label{e:ADMIT:CV}
\E[w(\thetab)^2] / \E[w(\thetab)]^2
\end{align}
with respect to $\eta_h$ $(h=1,\ldots,H)$, where:
\[
\E[w(\thetab)^m] = \frac{1}{N_p} \sum_{i=1}^{N_p} \sum_{h=1}^H
\eta_h \, w(\thetab_h^{[i]})^m \quad (m=1,2)
\comma
\]
and:
\[
w(\thetab_h^{[i]})
= \frac{k(\thetab_h^{[i]})}{\sum_{l=1}^H \eta_l \, \dSt_d(\thetab_h^{[i]} \,|\, \mub_l, \Sigmab_l, \nu)}
\point
\]

\emph{Comment:} Minimization of~\eqref{e:ADMIT:CV} is time consuming.
The reason is that this concerns the optimization of a non-linear function
of $\eta_h$ $(h=1,\ldots,H)$ where $H$ takes the values $2,3,\ldots$ in the consecutive iterations of the algorithm.
Evaluating the function itself requires already $NH$ evaluations of the kernel
and $NH^2$ evaluations of the \Student densities. The
computation of (analytically evaluated) derivatives of
the function with respect to $\eta_h$ $(h=1,\ldots,H)$ takes even more time. One way to reduce the amount
of computing time required for the construction of the approximation is to
use different numbers of draws in different steps. One can use a relatively small
sample of $N_p$ draws for the optimization of the mixing probabilities and a large sample of $N_s$ draws in order
to evaluate the quality of the current candidate mixture at each iteration (in the sense of the coefficient
of variation of the corresponding importance sampling weights) and in order to obtain an initial value for the algorithm
that is used to optimize the weight function (that yields the mode of
a new \Student component in the mixture). Note that it is not necessary to find the
\emph{globally optimal} values of the mixing probabilities; a \emph{good} approximation
to the target density is all that is required.

\item[Step 2c -- Draw from the mixture] Draw a sample
of $N_s$ points $\thetab^{[i]}$ $(i=1,\ldots,N_s)$ from the new mixture
of \Student distributions, $q(\thetab) = \sum_{h=1}^H \eta_h \, \dSt_d(\thetab \,|\, \mub_h, \Sigmab_h, \nu)$, and go to
step 1; in order to draw a point from the density $q(\thetab)$ first use a draw from
the uniform distribution $\Un(0,1)$ to
determine which component $\dSt_d(\thetab \,|\, \mub_h, \Sigmab_h, \nu)$ is chosen, and then
draw from this $d$-dimensional \Student distribution.
\end{description}

\emph{Comment:} It may occur that one is dissatisfied with diagnostics like the coefficient of variation of
the importance sampling weights corresponding to the final candidate density
resulting from the procedure above. In that case the user may start all
over again the procedure with a larger number of points $N_s$. The idea behind this strategy is that the larger $N_s$, the easier
it is for the method to detect and approximate the shape of
the target density kernel, and to specify the \Student distributions
of the mixture adequately.

If the region of integration $\Theta \subseteq \mathbb{R}^d$ is bounded, it may
occur in step 2 that $w(\thetab)$ attains
its maximum at a boundary of the integration region. In this case minus the inverse Hessian of $\log w(\thetab)$ evaluated
at its mode $\mub_h$ may be a very poor scale matrix; in fact this matrix may not even be positive
definite. In such situations, $\mub_h$ and $\Sigmab_h$ are obtained as the estimated mean and covariance based
on a subset of draws corresponding to a certain percentage of largest weights. More precisely, $\mub_h$ and
$\Sigmab_h$ are obtained using the
sample $\{\thetab^{[i]} \,|\, i=1,\ldots,N_s\}$ from $q(\thetab)$ we already have:
\begin{align}\label{e:ADMIT:SigmahIS}
\begin{split}
\mub_h &=
\sum_{j \in J_{c}}
\frac{w(\thetab^{[j]})}{\sum_{j \in J_{c}} w(\thetab^{[j]})}
\thetab^{[j]}\\
\Sigmab_h &=
\sum_{j \in J_{c}}
\frac{w(\thetab^{[j]})}{\sum_{j \in J_{c}} w(\thetab^{[j]})}
(\thetab^{[j]} - \mub_h) (\thetab^{[j]} - \mub_h)'
\comma
\end{split}
\end{align}
where $J_c$ denotes the set of indices corresponding to the $c$ percents of the
largest weights in the sample $\{ w(\thetab^{[i]}) \,|\, i=1,\ldots,N_s \}$. Since our aim
is to detect regions with too little candidate probability mass (e.g., a distant mode), the
percentage $c$ is typically a low value, i.e., 5\%, 15\% or 30\%. Moreover, the estimated $\Sigmab_h$ can
be scaled by a given factor for robustness. Different percentages and scaling factors could be used together, leading to
different coefficients of variation at each step of the adaptive procedure. The matrix leading to the smallest
coefficient of variation could then be selected as the scale matrix $\Sigmab_h$ for the new mixture component.

Once the adaptive mixture of \Student distributions has been fitted to the target density $p(\thetab)$ through the
kernel function $k(\thetab)$, the approximation $q(\thetab)$ is used in importance sampling
or in the independence chain Metropolis-Hastings (\MH) algorithm to obtain quantities of interest for the target
density $p(\thetab)$ itself.

\subsection{Background on importance sampling}
\label{ss:ADMIT:IS}

Importance sampling, due to \citet{Hammersley:Handscomb:65}, was introduced in econometrics and
statistics by \citet{Kloek:VanDijk:78}. It is based on the following relationship:
\begin{align}~\label{e:ADMIT:IS}
\E_p \big[ g(\thetab) \big]
= \frac{\int g(\thetab) p(\thetab) \text{d} \thetab}{\int p(\thetab) \text{d} \thetab}
= \frac{\int g(\thetab) w(\thetab) q(\thetab) \text{d} \thetab}{\int w(\thetab) q(\thetab) \text{d} \thetab}
= \frac{\E_q \big[ g(\thetab) w(\thetab) \big]}{\E_q \big[ w(\thetab) \big]}
\comma
\end{align}
where $g(\thetab)$ is a given (integrable with respect to $p$) function, $w(\thetab) = k(\thetab)/q(\thetab)$,
$\E_p$ denotes the expectation with respect to the target density $p(\thetab)$ and $\E_q$ denotes
the expectation with respect to the (importance) approximation $q(\thetab)$. The importance sampling
estimator of $\E_p\big[ g(\thetab) \big]$ is then obtained as the sample counter-part of the right-hand
side of~\eqref{e:ADMIT:IS}:
\begin{align}~\label{e:ADMIT:ghat}
\widehat g =
\frac{\sum_{i=1}^N g(\thetab^{[i]}) w(\thetab^{[i]})}{\sum_{i=1}^N w(\thetab^{[i]})}
\comma
\end{align}
where $\{ \thetab^{[i]} \,|\, 1,\ldots,N \}$ is a sample of draws from the importance density $q(\thetab)$. Under certain
conditions \citep[see][]{Geweke:89}, $\widehat g$ is a consistent estimator of $\E_p\big[ g(\thetab) \big]$. The choice
of the function $g(\thetab)$ allows to obtain different quantities of interest for $p(\thetab)$. For instance,
the mean estimate of $p(\thetab)$, denoted by $\overline{\thetab}$, is obtained
with $g(\thetab) = \thetab$; the covariance matrix estimate is obtained using
$g(\thetab) = (\thetab - \overline{\thetab}) (\thetab - \overline{\thetab})'$;
the estimated probability that $\thetab$ belongs to a domain $D \subseteq \Theta$ using
$g(\thetab) = \Ind{\thetab \in D}$, where $\Ind{\myBigDot}$ denotes the indicator function which is equal
to one if the constraint holds and zero otherwise.

\subsection{Background on the independence chain Metropolis-Hastings algorithm}
\label{ss:ADMIT:MH}

The Metropolis-Hastings (\MH) algorithm is a Markov chain Monte Carlo (\MCMC) approach
that has been introduced by \citet{Metropolis:Rosenbluth:Rosenbluth:Teller:Teller:53} and
generalized by \citet{Hastings:70}. \MCMC methods construct a Markov chain converging to a
target distribution $p(\thetab)$. After a \emph{burn-in} period, which is required to make the influence
of initial values negligible, draws from the Markov chain are considered as (correlated) draws from the
target distribution itself.

In the independence chain \MH algorithm, a Markov chain of length $N$ is constructed by
the following procedure. First, one
chooses a feasible initial state $\thetab^{[0]}$. Then, one repeats the following steps $N$ times
(for $i=1,\ldots,N$). A candidate value $\thetab^\star$ is drawn from the candidate density
$q(\thetab^\star)$ and a random variable $U$ is drawn
from the uniform distribution $\Un(0,1)$. Then the acceptance probability:
\[
\xi(\thetab^{[i-1]},\thetab^\star)
= \min
\left\{
\frac{w(\thetab^\star)}{w(\thetab^{[i-1]})},1
\right\}
\]
is computed, where $w(\thetab) = k(\thetab)/q(\thetab)$, $k(\thetab)$ being a kernel
of the target density $p(\thetab)$. If $U<\xi(\thetab^{[i-1]},\thetab^\star)$, the transition to the candidate value
is accepted, i.e., $\thetab^{[i]} = \thetab^\star$. Otherwise the transition is rejected, and the next state is again
$\thetab^{[i]} = \thetab^{[i-1]}$.

\section{Illustration I: the Gelman-Meng distribution}
\label{s:GM}

This section presents the functions provided by the \proglang{R} package \AdMit with an
illustration of a bivariate bimodal distribution. This distribution belongs to the class of conditionally Normal
distributions proposed by \citet{Gelman:Meng:91} with the property that the joint density is
not Normal. In the notation of the previous section, we have $\thetab=(X_1 \ X_2)'$.

Let $X_1$ and $X_2$ be two random variables, for which $X_1$ is Normally distributed given $X_2$ and
vice versa. Then, the joint distribution, after location and scale transformations in each variable, can
be written as \citep[see][]{Gelman:Meng:91}:
\begin{align}~\label{e:GM:kernel}
p(x_1,x_2) \propto \exp
\left( -\half [A x_1^2 x_2^2 + x_1^2 + x_2^2 - 2 B x_1 x_2 - 2 C_1 x_1 - 2 C_2 x_2] \right)
\comma
\end{align}
where $A$, $B$, $C_1$ and $C_2$ are constants. Equation~\eqref{e:GM:kernel} can be rewritten as:
\[
p(x_1,x_2) \propto \exp
\left( -\half \big[A x_1^2 x_2^2 + (x-\mub)' \Sigmab^{-1} (x-\mub) \big] \right)
\comma
\]
with:
\[
\mub =
\left( \frac{BC_2+C_1}{1-B^2} \quad \frac{BC_1+C_2}{1-B^2} \right)'
\quad \text{and} \quad
\Sigmab^{-1} =
\begin{pmatrix}
1  & -B\\
-B & 1
\end{pmatrix}
\comma
\]
so the term $Ax_1^2x_2^2$ causes deviations from the bivariate Normal distribution. In what follows, we
consider the symmetric case in which $A=1$, $B=0$, $C_1=C_2=3$.

The core function provided by the \proglang{R} package \AdMit is the function \code{AdMit}. The arguments
of the function are the following:
\begin{Code}
AdMit(KERNEL, mu0, Sigma0 = NULL, control = list(), ...)
\end{Code}
\code{KERNEL} is a kernel function $k(\thetab)$ of the target density $p(\thetab)$ on which the
approximation is constructed. This function must contain the logical argument \code{log}. When \code{log = TRUE}, the
function \code{KERNEL} returns the (natural) logarithm value of the
kernel function; this is used for numerical stability. \code{mu0} is the
starting value of the first stage
optimization $\mub_1 = \arg \max_{\thetab \in \Theta} \log k(\thetab)$; it is a vector whose
length corresponds to the length of the first argument in \code{KERNEL}. If one experiences misconvergence
of the first stage optimization, one could first use an alternative (robust) optimization algorithm and use its
output for \code{mu0}. For instance, the \code{DEoptim} function provided by the \proglang{R} package
\DEoptim \citep{DEoptim} performs the optimization (minimization) of a function using an
evolutionary (genetic) approach. \code{Sigma0} is the (symmetric positive definite) scale matrix
of the first component. If a matrix is provided by the user, then it is used as the scale matrix of the first component and
\code{mu0} is used as the mode of the first component. \code{control} is a list
of tuning parameters. The most important parameters are: \code{Ns} (default: \code{1e+05}), the number
of draws used for evaluating the importance sampling weights; \code{Np} (default: \code{1e+03}),
the number of draws used for optimizing the mixing probabilities; \code{CVtol} (default: \code{0.1}), the tolerance
of the relative change of the coefficient of variation; \code{df} (default: \code{1}), the degrees of freedom
of the mixture components; \code{Hmax} (default: \code{10}), the maximum number of components of the
mixture; \code{IS} (default: \code{FALSE}), indicates if the
scale matrices $\Sigmab_h$ should always be estimated by importance sampling
as in~\eqref{e:ADMIT:SigmahIS} without first trying to compute minus
the inverse Hessian; \code{ISpercent} (default: \code{c(0.05, 0.15, 0.30)}), a vector of
percentages of largest weights used in the importance
sampling approach; \code{ISscale} (default: \code{c(1, 0.25, 4)}), a vector of scaling
factors used to rescale the scale
matrix obtained by importance sampling. Hence, when the argument \code{IS = TRUE}, nine scale matrices
are constructed by default
and the matrix leading to the smallest coefficient of variation is selected by the adaptive
mixture procedure as $\Sigmab_h$.
For details on the other \code{control} parameters, the reader is referred to the
documentation file of \code{AdMit} (by
typing \code{?AdMit}). Finally, the last argument of \code{AdMit} is \code{\ldots} which allows
the user to pass additional arguments to the function \code{KERNEL}. In econometric models for instance,
the kernel may depend on a vector of
observations $\yb = \ve y '$ which can be passed to the function \code{KERNEL} via this argument.

For the numerical optimization of the mode $\mub_h$ and the estimation of the scale
matrix $\Sigmab_h$ (i.e., when the \code{control} parameter \code{IS = FALSE}),
the function \code{optim} is used with the
option \code{BFGS} (the function \code{nlminb} cannot be used since it does
not estimate the Hessian matrix at optimum). If
the optimization procedure does not converge, the algorithm automatically
switches to the \code{Nelder-Mead} approach which is more robust but slower. If still
misconvergence occurs or if the Hessian matrix at optimum is not symmetric positive definite, the algorithm
automatically switches to the importance sampling approach for this component.

For the numerical optimization of the mixing probabilities $\eta_h$ $(h=1,\ldots,H)$, we rely on
the function \code{nlminb} (for speed purposes) and apply the optimization on a reparametrized domain. More
precisely, we optimize $(H-1)$ components in $\mathbb{R}^{(H-1)}$ instead of $H$ components in $[0,1]^H$
with the summability constraint $\sum_{h=1}^H \eta_h$. If the optimization process does not converge, then the algorithm uses
the function \code{optim} with method \code{Nelder-Mead} (or method \code{BFGS} for univariate optimization) which
is more robust but slower. If still
misconvergence occurs, the starting value is kept as the output of the procedure. The starting value corresponds
to a mixing probability \code{weightNC} for $\eta_h$ while the probabilities $\eta_1,\ldots,\eta_{H-1}$ are the
probabilities of the previous mixture scaled by (\code{1 - weightNC}). The \code{control} parameter \code{weightNC} is
set to \code{0.1} by default, i.e., a 10\% probability is assigned to the new mixture component as a starting value. Finally,
note that \code{AdMit} uses \proglang{C} and analytically evaluated derivatives to speed up
the numerical optimization.

Let us come back to our bivariate conditionally Normal distribution. First, we need to define
the kernel function in~\eqref{e:GM:kernel}. This is achieved as follows:
<<>>=
GelmanMeng <- function(x, A = 1, B = 0, C1 = 3, C2 = 3, log = TRUE)
{
  if (is.vector(x))
    x <- matrix(x, nrow = 1)
  r <- -0.5 * (A * x[,1]^2 * x[,2]^2 + x[,1]^2 + x[,2]^2 - 2 * B * x[,1] * x[,2] - 2 * C1 * x[,1] - 2 * C2 * x[,2])
  if (!log)
    r <- exp(r)
  as.vector(r)
}
@
Note that the argument \code{log} is set to \code{TRUE} by default so that the
function outputs the (natural) logarithm of the kernel function. Moreover, the function
is vectorized to speed up the computations. The argument \code{x} is therefore a matrix and the
function outputs a vector. We strongly advise the user to implement the kernel function in this fashion.
A plot of \code{GelmanMeng} may be obtained as follows:
<<>>=
PlotGelmanMeng <- function(x1, x2)
{
  GelmanMeng(cbind(x1, x2), log = FALSE)
}
x1 <- x2 <- seq(from = -1.0, to = 6.0, by = 0.05)
z <- outer(x1, x2, FUN = PlotGelmanMeng)
image(x1, x2, z, las = 1, col = gray((20:0)/20),
      cex.axis = 1.1, cex.lab = 1.2,
      xlab = expression(X[1]), ylab = expression(X[2]))
box()
abline(a = 0, b = 1, lty = "dotted")
@
The plot of \code{GelmanMeng} is displayed in \myFigure{f:GM:kernel}. 
We notice the bimodal banana shape of the kernel function.
\setkeys{Gin}{width=.7\textwidth}
\begin{figure}[tbh]
\begin{center}
<<fig=TRUE,height=7,width=7,echo=FALSE>>=
PlotGelmanMeng <- function(x1, x2)
{
  GelmanMeng(cbind(x1, x2), log = FALSE)
}
x1 <- x2 <- seq(from = -1.0, to = 6.0, by = 0.05)
z <- outer(x1, x2, FUN = PlotGelmanMeng)
image(x1, x2, z, las = 1, col = gray((20:0)/20),
      cex.axis = 1.1, cex.lab = 1.2,
      xlab = expression(X[1]), ylab = expression(X[2]))
box()
abline(a = 0, b = 1, lty = "dotted")
@
\caption[]{Plot of the \citet{Gelman:Meng:91} kernel function.}
\label{f:GM:kernel}
\end{center}
\end{figure}

Let us now use the function \code{AdMit} to find a suitable approximation for the density function
$p(\thetab)$ whose kernel is~\eqref{e:GM:kernel}. We set the seed of the pseudo-random
number generator to a given number and use the starting value \code{mu0 = c(0.0, 0.1)} for the first stage
optimization. The result of the function is assigned to the object \code{outAdMit} and printed out:
<<>>=
set.seed(1234)
outAdMit <- AdMit(KERNEL = GelmanMeng, mu0 = c(0.0, 0.1))
print(outAdMit)
@
The output of the function \code{AdMit} is a list. The first
component is \code{CV}, a vector of length $H$ which gives the value of the coefficient
of variation at each step of the adaptive fitting procedure. The second component is \code{mit}, a list which
consists of four components giving information
on the fitted mixture of \Student distributions: \code{p} is a vector 
of length $H$ of mixing probabilities, \code{mu} is a $H\times d$ matrix
whose rows give the modes of the mixture components, \code{Sigma} is a $H \times d^2$ matrix 
whose rows give the scale matrices (in vector form) of the mixture components and \code{df} is the degrees of
freedom of the \Student components. The third component of the list returned by \code{AdMit} is \code{summary}. This
is a data frame containing information on the adaptive fitting procedure: \code{H} is
the component's number; \code{METHOD.mu} indicates which algorithm is used 
to estimate the mode and the scale matrix of 
the component (i.e., \code{USER}, \code{BFGS}, \code{Nelder-Mead} or \code{IS}); \code{TIME.mu} gives
the computing time required for this optimization; \code{METHOD.p} gives
the method used to optimize the mixing probabilities (i.e., \code{NONE}, 
\code{NLMINB}, \code{BFGS} or \code{Nelder-Mead}); \code{TIME.p} gives the computing time required for
this optimization; \code{CV} gives the coefficient of variation of the importance sampling weights. 
When importance sampling is used (i.e., \code{IS = TRUE}), \code{METHOD.mu} is of the 
type \code{IS 0.05-0.25} indicating in this particular case, that importance sampling 
is used with the 5\% largest weights and with a scaling factor of 0.25. Hence, if the \code{control} parameters 
\code{ISpercent} and \code{ISscale} are vectors of sizes $d_1$ and $d_2$, then $d_1 d_2$ matrices are 
considered for each component \code{H}, and the matrix leading to the smallest 
coefficient of variation is kept as the scale matrix $\Sigmab_h$ for this component. 
Time outputs \code{TIME.mu} and \code{TIME.p} are provided since it might be useful, as a 
robustness check, to see the computing time required for separate ingredients of 
the fitting procedure, that is the optimization of the modes and the optimization of 
the mixing probabilities. A very long computing time might indicate a numerical failure 
at some stage of the optimization process. 

For the kernel function \code{GelmanMeng}, the approximation constructs a mixture of four components. The 
computing time required for the construction of the 
approximation is 4.4 seconds. The value of the
coefficient of variation decreases from 4.8224 to 0.8315. A plot of the four-component approximation is displayed 
in \myFigure{f:GM:approximation}. This graph is produced using the function \code{dMit} which
returns the density of the mixture given by the output \code{outAdMit\$mit}:
<<>>=
PlotMit <- function(x1, x2, mit)
{
  dMit(cbind(x1, x2), mit = mit, log = FALSE)
}
z <- outer(x1, x2, FUN = PlotMit, mit = outAdMit$mit)
image(x1, x2, z, las = 1, col = gray((20:0)/20),
      cex.axis = 1.1, cex.lab = 1.2,
      xlab = expression(X[1]), ylab = expression(X[2]))
box()
abline(a = 0, b = 1, lty = "dotted")
@
\setkeys{Gin}{width=.7\textwidth}
\begin{figure}[tbh]
\begin{center}
<<fig=TRUE,height=7,width=7,echo=FALSE>>=
PlotMit <- function(x1, x2, mit)
{
  dMit(cbind(x1, x2), mit = mit, log = FALSE)
}
z <- outer(x1, x2, FUN = PlotMit, mit = outAdMit$mit)
image(x1, x2, z, las = 1, col = gray((20:0)/20),
      cex.axis = 1.1, cex.lab = 1.2,
      xlab = expression(X[1]), ylab = expression(X[2]))
box()
abline(a = 0, b = 1, lty = "dotted")
@
\caption[]{Plot of the four-component \Student mixture approximation estimated by the function \code{AdMit}.}
\label{f:GM:approximation}
\end{center}
\end{figure}
The plot suggests that the four-component mixture provides a good approximation of the density function
whose kernel is~\eqref{e:GM:kernel}. 

We can also use the mixture information 
\code{outAdMit\$mit} to display each of the mixture components separately:
<<>>=
par(mfrow = c(2,2))
for (h in 1:4)
{
  mith <- list(p = 1,
               mu = outAdMit$mit$mu[h,,drop = FALSE],
               Sigma = outAdMit$mit$Sigma[h,,drop = FALSE],
               df = outAdMit$mit$df)
  z <- outer(x1, x2, FUN = PlotMit, mit = mith)
  image(x1, x2, z, las = 1, col = gray((20:0)/20),
        cex.axis = 1.1, cex.lab = 1.2,
        xlab = expression(X[1]), ylab = expression(X[2]))
  box()
  abline(a = 0, b = 1, lty = "dotted")
  title(main = paste("component nr.", h))
}
@
Plots of the four components are displayed in \myFigure{f:GM:components}.
\setkeys{Gin}{width=.7\textwidth}
\begin{figure}[tbh]
\begin{center}
<<fig=TRUE,height=7,width=7,echo=FALSE>>=
par(mfrow = c(2,2))
for (h in 1:4)
{
  mith <- list(p = 1,
               mu = outAdMit$mit$mu[h,,drop = FALSE],
               Sigma = outAdMit$mit$Sigma[h,,drop = FALSE],
               df = outAdMit$mit$df)
  z <- outer(x1, x2, FUN = PlotMit, mit = mith)
  image(x1, x2, z, las = 1, col = gray((20:0)/20),
        cex.axis = 1.1, cex.lab = 1.2,
        xlab = expression(X[1]), ylab = expression(X[2]))
  box()
  abline(a = 0, b = 1, lty = "dotted")
  title(main = paste("component nr.", h))
}
@
\caption[]{\Student components of the four-component mixture
approximation estimated by the function \code{AdMit}.}
\label{f:GM:components}
\end{center}
\end{figure}

Once the adaptive mixture of \Student distributions is fitted to the density $p(\thetab)$
using a kernel $k(\thetab)$, the approximation $q(\thetab)$ provided by \code{AdMit} is used as the importance
sampling density in importance sampling or as the candidate density 
in the independence chain \MH algorithm.

The first function provided by the \proglang{R} package \AdMit which allows to find quantities of interest for the density
$p(\thetab)$ using the output \code{outAdMit\$mit} of \code{AdMit} is the function \code{AdMitIS}.
This function performs importance sampling using the mixture approximation as the importance 
density (see \mySection{ss:ADMIT:IS}). The arguments of the function \code{AdMitIS} are the following:
\begin{Code}
AdMitIS(N = 1e+05, KERNEL, G = function(theta){theta}, mit = list(), ...) 
\end{Code}
\code{N} is the number of draws used in importance
sampling; \code{KERNEL} is a kernel function $k(\thetab)$ of the target
density $p(\thetab)$; \code{G} is the function
$g(\thetab)$ in~\eqref{e:ADMIT:IS}; \code{mit} is a list
providing information on the mixture approximation (i.e., typically the component \code{mit} in the output of
the \code{AdMit} function); \code{\ldots} allows additional parameters to be passed to
the function \code{KERNEL} and/or \code{G}.

Let us apply the function \code{AdMitIS} to the
kernel \code{GelmanMeng} using the approximation \code{outAdMit\$mit}:
<<>>=
set.seed(1234)
outAdMitIS <- AdMitIS(KERNEL = GelmanMeng, mit = outAdMit$mit)
print(outAdMitIS)
@
The output of the function \code{AdMitIS} is a list. The first component is
\code{ghat}, the importance sampling estimator of $\E_p\big[g(\thetab)\big]$ in~\eqref{e:ADMIT:ghat}. This is a
vector whose length corresponds to the length of the output of the function \code{G}. The second component
is \code{NSE}, a vector containing the numerical standard
errors (i.e., the square root of the variance of the estimates that can be expected if the simulations
were to be repeated) of the components of \code{ghat}. The
third component is \code{RNE}, a vector containing the relative numerical
efficiencies of the components of \code{ghat} (i.e., the ratio
between an estimate of the variance of an estimator based on direct
sampling and the importance sampling estimator's estimated variance with
the same number of draws). \code{RNE} is an indicator of the
efficiency of the chosen importance function; if target and importance densities coincide,
\code{RNE} equals one, whereas a very poor importance density will have a \code{RNE} close
to zero. Both \code{NSE} and \code{RNE} are estimated by the method given in \citet{Geweke:89}. For
estimating $\E_p[g(\thetab)]$ the $N$ candidate draws are approximately as `valuable' as
\code{RNE} $\times$ $N$ independent draws from the target would be.

The computing time required to perform importance sampling on \code{GelmanMeng} using
the four-component mixture \code{outAdMit\$mit} is 0.7 seconds, where most part of
the computing time is required for the $N$ evaluations of the function \code{KERNEL} at
the sampled values $\{ \thetab^{[i]} \,|\, i=1,\ldots,N \}$. The true values for
$\E_p(X_1)$ and $\E_p(X_2)$ are 1.459. We notice that the importance sampling estimates are close to the true values and we
note the good efficiency of the estimation.

By default, the function \code{G} is \code{function(theta)\{theta\}} so that the function
outputs a vector containing the mean estimates for the components of $\thetab$. Alternative
functions may be provided
by the user to obtain other quantities of interest for $p(\thetab)$. The only requirement is
that the function outputs a matrix. For instance, to estimate the
covariance matrix of $\thetab$, we could define the following function:
<<>>=
G.cov <- function(theta, mu)
{
  G.cov_sub <- function(x)
    (x - mu) %*% t(x - mu)
  theta <- as.matrix(theta)
  tmp <- apply(theta, 1, G.cov_sub)
  if (length(mu) > 1)
    t(tmp)
  else
    as.matrix(tmp)
}
@
Applying the function \code{AdMitIS} with \code{G.cov} leads to:
<<>>=
set.seed(1234)
outAdMitIS <- AdMitIS(KERNEL = GelmanMeng, G = G.cov, mit = outAdMit$mit, mu = c(1.459, 1.459))
print(outAdMitIS)
V <- matrix(outAdMitIS$ghat, 2, 2)
print(V)
@
\code{V} is the covariance matrix estimate. For this estimation, we have used
the real mean values, i.e., \code{mu = c(1.459, 1.459)}, so that \code{NSE} and \code{RNE} of
the covariance matrix elements are correct. In general, those mean values are unknown and we have to
resort to the importance sampling estimates. In this case, the numerical standard errors of the estimated covariance
matrix elements are (generally slightly) downward biased.

The function \code{cov2cor} can be used to obtain the correlation matrix corresponding to the covariance matrix:
<<>>=
cov2cor(V)
@

The second function provided by the \proglang{R} package \AdMit which allows to find quantities of
interest for the target density $p(\thetab)$ using the output \code{outAdMit$mit} of \code{AdMit} is the
function \code{AdMitMH}. This function uses the mixture approximation as the candidate density in
the independence chain \MH algorithm (see \mySection{ss:ADMIT:MH}). The arguments
of the function \code{AdMitMH} are the following:
\begin{Code}
AdMitMH(N = 1e+05, KERNEL, mit = list(), ...)
\end{Code}
\code{N} is the length of the \MCMC sequence of draws; \code{KERNEL} is a kernel
function $k(\thetab)$ of the target density $p(\thetab)$; \code{mit} is a list
providing information on the mixture approximation (i.e., traditionally the component \code{mit} in the output of
the function \code{AdMit}); \code{\ldots} allows additional parameters to be passed to
the function \code{KERNEL}.

Let us apply the function \code{AdMitMH} to the
kernel \code{GelmanMeng} using the approximation \code{outAdMit\$mit}:
<<>>=
set.seed(1234)
outAdMitMH <- AdMitMH(KERNEL = GelmanMeng, mit = outAdMit$mit)
print(outAdMitMH)
@
The output of the function \code{AdMitMH} is a list of two components. The first component is
\code{draws}, a $N \times d$ matrix containing draws from the
target density $p(\thetab)$ in its rows. The second component is \code{accept}, the
acceptance rate of the independence chain \MH algorithm.

In our example, the computing time required to generate a \MCMC chain of
size \code{N = 1e+05} (i.e., the default value) takes 0.8 seconds. Note
that as for the function \code{AdMitIS}, the most important part of the computing time is required for evaluations of the
\code{KERNEL} function. Part of the \code{AdMitMH} function is implemented in \proglang{C} in order
to accelerate the generation of the \MCMC output. The rather high acceptance rate above 50\% suggests
that the mixture approximates the target density quite well.

The \proglang{R} package \pkg{coda} \citep{coda} can be used
to check the convergence of the \MCMC chain and obtain quantities of interest for $p(\thetab)$.
Here, for simplicity, we discard the first 1'000 draws
as a burn-in sample and transform the
output \code{outAdMitMH$draws} in a \code{mcmc} object using the function \code{as.mcmc} provided by
\pkg{coda}. A summary of the \MCMC chain can be obtained using \code{summary}:
<<echo=FALSE>>=
library("coda")
@
<<>>=
draws <- as.mcmc(outAdMitMH$draws[1001:1e5,])
colnames(draws) <- c("X1", "X2")
summary(draws)$stat
@
We note that the mean estimates are close to the values
obtained with the function \code{AdMitIS}. The relative numerical efficiency can be computed from
the output of the function \code{summary} by dividing the square of the (robust) numerical
standard error of the mean estimates (i.e., \code{Time-series SE}) by the square of the naive
estimator of the numerical standard error (i.e., \code{Naive SE}):
<<>>=
summary(draws)$stat[,3]^2 / summary(draws)$stat[,4]^2
@
These relative numerical efficiencies reflect the good quality of the candidate density in the independence
chain \MH algorithm.

Finally, note that for more flexibility, the functions \code{AdMitIS} and \code{AdMitMH} require the arguments
\code{N} and \code{KERNEL}. Therefore, the number of sampled values \code{N} in importance sampling
or in the independence chain \MH algorithm can be
different from the number of draws \code{Ns} used to fit the \Student mixture approximation. In addition,
the same mixture approximation can be used for different kernel functions. This can be useful, typically in Bayesian
times series econometrics, to update a joint posterior distribution with the arrival
of new observations. In this case, the previous mixture
approximation (i.e., fitted on a kernel function which is based on $T$ observations) can be used as
the candidate density to approximate the updated joint posterior
density which accounts for the new
observations (i.e., whose kernel function is based on $T+k$ observations where $k \geq 1$).

\section{Concluding remarks}
\label{s:CONCLUSION}

This paper presented the \proglang{R} package \AdMit which provides functions to approximate
and sample from a certain target distribution given only a kernel of the target density function.
The estimation procedure is fully automatic and thus avoids the time-consuming and
difficult task of tuning a sampling algorithm. The relevance of the package has been shown
in an example of a bivariate bimodal distribution. 

Interested reader are referred to \citet{Ardia:Hoogerheide:VanDijk:09} for a more complete description
of the \proglang{R} package \AdMit. In particular, we show the relevance of the AdMit procedure
through the Bayesian estimation of a mixture of \ARCH model fitted to foreign exchange log-returns data. The
methodology is compared to standard cases of importance sampling and the Metropolis-Hastings algorithm using
a naive candidate and with the Griddy-Gibbs approach. Both for investigating means and tails
of the joint posterior distribution the adaptive approach is preferable.

We believe that this approach may be applicable in many fields of research and hope that the \proglang{R} package \AdMit will be
fruitful for many researchers like econometricians or applied statisticians.

Finally, if you use \proglang{R} or \AdMit, please cite the software in publications.

\section*{Acknowledgments}

The authors acknowledge three anonymous reviewers and Achim Zeileis for numerous helpful 
suggestions that have led to substantial improvements of the paper. The first author is grateful to the 
Swiss National Science Foundation (under grant \#FN~PB~FR1-121441) for financial support. The third author 
gratefully acknowledges the financial assistance from the Netherlands Organization 
of Research (under grant \#400-07-703). 
Any remaining errors or shortcomings are the authors' responsibility.

\bibliography{REFERENCES}

\end{document}
%%
%% EOF
